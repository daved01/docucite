{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and setup\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "PDF_PATH = \"./data/Fluent Python.pdf\"\n",
    "# PDF_PATH = \"./data/Python summary.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document loader\n",
    "- Load the book.\n",
    "\n",
    "- Add chapter title to metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(PDF_PATH)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to split it well? How to do that in the final app?\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "CHUNK_SIZE = 1500\n",
    "CHUNK_OVERLAP = 150\n",
    "\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='5The expression self.__data[name]  is where a KeyError  exception may occur. Ideally, it should be handled\\nand an AttributeError  raised instead, because that’s what is expected from __getattr__ . The diligent reader\\nis invited to code the error handling as an exercise.\\n6The source of the data is JSON, and the only collection types in JSON data are dict  and list .Otherwise, fetch the item with the key name  from self.__data , and return the\\nresult of calling FrozenJSON.build()  on that.5\\nImplementing __dir__  suports the dir()  built-in, which in turns supports auto-\\ncompletion in the standard Python console as well as IPython, Jupyter Notebook,\\netc. This simple code will enable recursive auto-completion based on the keys in\\nself.__data , because __getattr__  builds FrozenJSON  instances on the fly—use‐\\nful for interactive exploration of the data.\\nThis is an alternate constructor, a common use for the @classmethod  decorator.\\nIf obj is a mapping, build a FrozenJSON  with it. This is an example of goose typ‐\\ning—see “Goose Typing” on page 442  if you need a refresher.\\nIf it is a MutableSequence , it must be a list,6 so we build a list  by passing each\\nitem in obj recursively to .build() .\\nIf it’s not a dict  or a list , return the item as it is.\\nA FrozenJSON  instance has the __data  private instance attribute stored under the\\nname _FrozenJSON__data , as explained in “Private and ‘Protected’ Attributes in\\nPython”  on page 382. Attempts to retrieve attributes by other names will trigger\\n__getattr__ . This method will first look if the self.__data  dict  has an attribute\\n(not a key!) by that name; this allows FrozenJSON  instances to handle dict  methods\\nsuch as items , by delegating to self.__data.items() . If self.__data  doesn’t have\\nan attribute with the given name , __getattr__  uses name  as a key to retrieve an item\\nfrom self.__data , and passes that item to FrozenJSON.build . This allows navigating', metadata={'source': './data/Fluent Python.pdf', 'page': 870})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[-200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage\n",
    "\n",
    "[] Add chapter name to metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "# Splitting format: chroma_<chunk_size>_<chunk_overlap>\n",
    "persist_directory = f\"data/chroma_{CHUNK_SIZE}_{CHUNK_OVERLAP}\"\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=docs, \n",
    "                                 embedding=embedding, \n",
    "                                 persist_directory=persist_directory\n",
    "                                 )\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25815\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some issues with retrieving results from this with similarity search. \n",
    "1) Duplicates can be there\n",
    "2) Context is missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What about dataclasses?\"\n",
    "docs = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "There are different types of search, such as `similarity_search`, `max_marginal_relevance_search`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "persist_directory = 'data/chroma'\n",
    "\n",
    "# Load embeddings from database\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using metadata\n",
    "docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=5,\n",
    "    # filter={\"\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './data/Fluent Python.pdf', 'page': 60}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1000].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, the book does discuss testing with Pytest.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "\n",
    "\n",
    "# result = qa_chain({\"query\": question})\n",
    "# result[\"result\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. Check in if the user understood the answer if appropriate. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Threads are an inefficient model for concurrency because they make code hard to reason about and lack constraints, which can lead to difficulty in managing large-scale concurrency.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Why are threads an inefficient model for concurrency?\"\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
